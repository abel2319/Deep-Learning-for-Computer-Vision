{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ca18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "#from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from IPython.display import Video\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47251406",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = deeplake.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670ed80",
   "metadata": {},
   "source": [
    "We've download the YouTube video (`https://www.youtube.com/watch?v=73fz_uK-vhs`) to directory `\"data\"`, so let's start by preparing that.\n",
    "\n",
    "**Task 4.7.1:** Create a path object for the `\"data\"` directory. Use the `Path` class from `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = \"lupita_nyongo.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffada949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables you've already defined will be helpful\n",
    "print(data_dir)\n",
    "print(video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = data_dir / video_name\n",
    "\n",
    "print(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video\n",
    "Video(input_video, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dir = data_dir / \"extracted_frames\"\n",
    "\n",
    "frames_dir.mkdir(exist_ok=True)\n",
    "print(frames_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(input_video)\n",
    "frame_rate = round(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "print(f\"Frame rate: {frame_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66159e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 6  # Extract every sixth frame from the video\n",
    "frame_count = 0\n",
    "\n",
    "print(\"Start extracting individual frames...\")\n",
    "while True:\n",
    "    # read the next frame from the video_capture\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Finished!\")\n",
    "        break  # Break the loop if there are no more frames\n",
    "\n",
    "    # Save frames at every 'interval' frames\n",
    "    if frame_count % interval == 0:\n",
    "        frame_path = frames_dir / f\"frame_{frame_count}.jpg\"\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = data_dir / \"images\"\n",
    "images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf194df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lupita_dir = images_dir / \"lupita\"\n",
    "# Now create `lupita` directory\n",
    "lupita_dir.mkdir(exist_ok=True)\n",
    "\n",
    "christoph_dir = images_dir / \"christoph\"\n",
    "# Now create `christoph` directory\n",
    "christoph_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lupita_imgs = [\n",
    "    \"frame_3438.jpg\",\n",
    "    \"frame_3486.jpg\",\n",
    "    \"frame_3852.jpg\",\n",
    "    \"frame_4062.jpg\",\n",
    "    \"frame_4914.jpg\",\n",
    "    \"frame_4866.jpg\",\n",
    "]\n",
    "\n",
    "christoph_imgs = [\n",
    "    \"frame_54.jpg\",\n",
    "    \"frame_66.jpg\",\n",
    "    \"frame_72.jpg\",\n",
    "    \"frame_108.jpg\",\n",
    "    \"frame_186.jpg\",\n",
    "    \"frame_246.jpg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lupita_img_paths = [frames_dir / i for i in lupita_imgs]\n",
    "christoph_img_paths = [frames_dir / i for i in christoph_imgs]\n",
    "\n",
    "print(\"Number of Lupita images:\", len(lupita_img_paths))\n",
    "print(\"Number of Christoph images:\", len(christoph_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, figsize=(10, 8))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(Image.open(lupita_img_paths[i]))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 6, figsize=(10, 8))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(Image.open(christoph_img_paths[i]))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83139ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy selected images of Lupita over to `lupita` directory\n",
    "for image_path in lupita_img_paths:\n",
    "    shutil.copy(image_path, lupita_dir)\n",
    "\n",
    "# Copy selected images of Christoph over to `christoph` directory\n",
    "for image_path in christoph_img_paths:\n",
    "    shutil.copy(image_path, christoph_dir)\n",
    "\n",
    "print(\"Number of files in lupita directory:\", len(list(lupita_dir.iterdir())))\n",
    "print(\"Number of files in christoph directory:\", len(list(christoph_dir.iterdir())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, min_face_size=40)\n",
    "\n",
    "print(f\"MTCNN min face size: {mtcnn.min_face_size}\")\n",
    "print(f\"MTCNN keeping all faces: {mtcnn.keep_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_filename = \"frame_4866.jpg\"\n",
    "sample_image_path = frames_dir / sample_image_filename\n",
    "\n",
    "sample_image = Image.open(sample_image_path)\n",
    "sample_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, probs, landmarks = mtcnn.detect(sample_image, landmarks=True)\n",
    "\n",
    "print(\"boxes:\", boxes)\n",
    "print(\"probs:\", probs)\n",
    "print(\"landmarks:\", landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(sample_image)\n",
    "\n",
    "for box, landmark in zip(boxes, landmarks):\n",
    "    rect = plt.Rectangle(\n",
    "        (box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, color=\"blue\"\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    for point in landmark:\n",
    "        ax.plot(point[0], point[1], marker=\"o\", color=\"red\")\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6648e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
    "\n",
    "print(f\"InceptionResnet weight set: {resnet.pretrained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5805ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(images_dir)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {i: c for c, i in dataset.class_to_idx.items()}\n",
    "\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e524c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "print(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf90314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps name to list of their embeddings\n",
    "name_to_embeddings = {name: [] for name in idx_to_class.values()}\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn(img, return_prob=True)\n",
    "    if face is not None and prob >= 0.90:\n",
    "        emb = resnet(face[0].unsqueeze(0))\n",
    "        name_to_embeddings[idx_to_class[idx]].append(emb)\n",
    "\n",
    "print(name_to_embeddings.keys())\n",
    "print(type(name_to_embeddings[\"lupita\"]))\n",
    "print(type(name_to_embeddings[\"christoph\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_lupita = torch.stack(name_to_embeddings['lupita'])\n",
    "embeddings_christoph = torch.stack(name_to_embeddings['christoph'])\n",
    "\n",
    "print(f\"Shape of stack of embeddings for Lupita: {embeddings_lupita.shape}\")\n",
    "print(f\"Shape of stack of embeddings for Christoph: {embeddings_christoph.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fe2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding_lupita = torch.mean(embeddings_lupita, dim=0)\n",
    "avg_embedding_christoph = torch.mean(embeddings_christoph, dim=0)\n",
    "\n",
    "print(f\"Shape of avg_embedding_lupita: {avg_embedding_lupita.shape}\")\n",
    "print(f\"Shape of avg_embedding_christoph: {avg_embedding_christoph.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260286b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [\"frame_2658.jpg\", \"frame_4614.jpg\", \"frame_972.jpg\", \"frame_30.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = [frames_dir / i for i in test_images]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(test_paths), figsize=(10, 8))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(Image.open(test_paths[i]))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import recognize_faces\n",
    "\n",
    "recognize_faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0687be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = [avg_embedding_lupita, avg_embedding_christoph]\n",
    "name_list = [\"lupita\", \"christoph\"]\n",
    "\n",
    "embedding_data = list(zip(embedding_list, name_list))\n",
    "\n",
    "print(embedding_data[0][0].shape, embedding_data[0][1])\n",
    "print(embedding_data[1][0].shape, embedding_data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9350c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognized_faces = []\n",
    "for test_img_path in test_paths:\n",
    "    # Call recognize_faces function using test_img_path\n",
    "    # and append the result to the list `recognized_faces`\n",
    "    recognized_faces.append(recognize_faces(test_img_path, embedding_data, mtcnn, resnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69093383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
